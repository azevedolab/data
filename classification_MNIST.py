# -*- coding: utf-8 -*-
"""Classifiers_for_MNIST_Dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18zVxfk9AgpfJNxkxb0AJTHd07o-qg60H

# **Classification Methods Applied to MNIST Dataset**
This Python code creates statistical learning models using the following classifiers: multi-layer Perceptron ([MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)), random forest ([RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)), and stochastic gradient descent ([SGDClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html)) from [Scikit-Learn](https://scikit-learn.org/stable/index.html)  [(Pedregosa et al., 2011)](https://www.jmlr.org/papers/volume12/pedregosa11a/pedregosa11a.pdf). It builds models to identify number 5 on the [MNIST](https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html) (Modified National Institute of Standards and Technology) dataset. You will find part of this code discussed in the book Géron 2023. This code generates receiver operating characteristic (ROC) curves and calculates the area under the curve (AUC) ([Fawcett, 2006](https://doi.org/10.1016/j.patrec.2005.10.010)) for models built using the previously highlighted classifiers. We employ the metrics recommended by [Walsh et al., 2021](https://doi.org/10.1038/s41592-021-01205-4) to analyze the predictive performance of the classifiers (e.g., precision, recall, and F1 score). This code relies on the following libraries: [Scikit-Learn](https://scikit-learn.org/stable/index.html) [(Pedregosa et al., 2011)](https://www.jmlr.org/papers/volume12/pedregosa11a/pedregosa11a.pdf), [NumPy](https://numpy.org/), [Pandas](https://pandas.pydata.org/), and [Matplotlib](https://matplotlib.org/).

References

Fawcett, T. An introduction to ROC analysis. Pattern Recognit. Lett., 2006, 27, 861–874.   [DOI](https://doi.org/10.1016/j.patrec.2005.10.010)

Géron, Aurélien. 2023. Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. 3rd ed. CA 95472: O’Reilly.

Pedregosa F, Varoquaux G, Gramfort A, Michel V, Thirion B, Grisel O, Blondel M, Prettenhofer P, Weiss R, Dubourg V, Verplas J, Passos A, Cournapeau D, Brucher M, Perrot M, Duchesnay E. Scikitlearn: Machine Learning in Python. J Mach Learn Res., 2011, 12, 2825-2830.   [PDF](https://www.jmlr.org/papers/volume12/pedregosa11a/pedregosa11a.pdf)

Walsh I, Fishman D, Garcia-Gasulla D, Titma T, Pollastri G; ELIXIR Machine Learning Focus Group, Harrow J, Psomopoulos FE, # Tosatto SCE. DOME: recommendations for supervised machine learning validation in biology. Nat Methods. 2021, 18(10), 1122-1127.   [DOI](https://doi.org/10.1038/s41592-021-01205-4)

# Function to Calculate Metrics (metrics)
This function focuses on evaluating the predictive performance of classifiers using metrics recommended by  [Walsh et al., 2021](https://doi.org/10.1038/s41592-021-01205-4).
"""
#
################################################################################
# Dr. Walter F. de Azevedo, Jr.                                                #
# https://github.com/azevedolab                                                #
# July 10, 2024                                                                #
################################################################################
#
def metrics(method_in,y,y_pred):

    # Import section
    from sklearn.metrics import confusion_matrix
    from sklearn.metrics import precision_score,recall_score,f1_score

    # Show metrics: confusion matrix, precision, recall, and F1 score
    print("Metrics for a model built using: ",method_in)
    print("Confusion Matrix: \n",confusion_matrix(y,y_pred))
    print("Precision:", round(precision_score(y,y_pred),3))
    print("Recall: ",round(recall_score(y,y_pred),3))
    print("F1 score:", round(f1_score(y,y_pred),3))

"""# ROC Class
This class generates ROC curves ([Fawcett, 2006](https://doi.org/10.1016/j.patrec.2005.10.010)) and determines the metrics ([Walsh et al., 2021](https://doi.org/10.1038/s41592-021-01205-4)) to evaluate the predictive performance of the classification models.
"""

# Define ROC() class
class ROC(object):

    # Define constructor method
    def __init__(self,method_list,X_in,y_in,y_score_list,clf_list,X_test,
                                                                      y_test):

        # Set up attributes
        self.method_list = method_list
        self.X_in = X_in
        self.y_in = y_in
        self.y_score_list = y_score_list
        self.clf_list = clf_list
        self.X_test = X_test
        self.y_test = y_test

    # Define performance() method
    def performance(self):
        from sklearn.model_selection import cross_val_predict

        # Looping through models to plot ROC and determine metrics
        for count, method_in in enumerate(self.method_list):

            # cross_val_predict() performs k-fold cross-validation
            y_train_p = cross_val_predict(self.clf_list[count],self.X_in,
                                          self.y_in,cv=5)
            y_test_p = cross_val_predict(self.clf_list[count],self.X_test,
                                         self.y_test,cv=5)

            # Call metrics() function
            metrics(self.method_list[count]+" (Training set)",self.y_in,
                    y_train_p)
            metrics(self.method_list[count]+" (Test set)",self.y_test,y_test_p)

    # Define curves() method
    def curves(self):

        # Import section
        import matplotlib.pyplot as plt
        from sklearn.model_selection import cross_val_predict
        from sklearn.metrics import precision_recall_curve,roc_auc_score,\
                                                                      roc_curve

        # Plotting
        plt.plot([0, 1], [0, 1], 'k:', label="Random classifier's ROC curve")

        # Looping through models to plot ROC and determine metrics
        for count, method_in in enumerate(self.method_list):

            # Get precisions,recalls,thresholds
            precisions,recalls,thresholds=precision_recall_curve(self.y_in,
                                                    self.y_score_list[count])

            # Determine fpr, tpr, and thresholds
            fpr,tpr,thresholds = roc_curve(self.y_in, self.y_score_list[count])

            # Determine area under the curve (AUC)
            auc = roc_auc_score(self.y_in, self.y_score_list[count])
            print("Area Under the Curve for "+self.method_list[count],
                                  round(auc,4))

            # Plot curve a model
            plt.plot(fpr, tpr, linewidth=2, label=self.method_list[count])

        plt.xlabel("False Positive Rate")
        plt.ylabel("Recall")
        plt.legend(loc='lower right')
        plt.grid()
        plt.show()
        plt.savefig("ROC_curves.png",dpi=600)

"""# MNIST Dataset
Here, we download the [MNIST](https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html) dataset. Then, we create the target vectors for this classification task. We use True for all 5s and False for all other digits.
"""

from sklearn.datasets import fetch_openml
mnist = fetch_openml('mnist_784', as_frame=False)
X, y = mnist.data, mnist.target
X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]
y_train_5 = (y_train == '5')
y_test_5 = (y_test == '5')
X_train_in,y_train_in = X_train,y_train_5
X_test_in,y_test_in = X_test,y_test_5

"""# MLPClassifier
This part of the code builds a classification model employing a multi-layer Perceptron classifier ([MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)) from [Scikit-Learn](https://scikit-learn.org/stable/index.html) [(Pedregosa et al., 2011)](https://www.jmlr.org/papers/volume12/pedregosa11a/pedregosa11a.pdf).
"""
msg_out = "\nClassification with Multilayer Perceptron"
print(msg_out,end="...")
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import cross_val_predict
mlp_clf = MLPClassifier(hidden_layer_sizes=(20,20,20),activation="relu",
solver="adam",alpha=0.0001,learning_rate="constant",learning_rate_init=0.001,
validation_fraction=0.1,max_iter=100000,shuffle=True,random_state=None)
pipeline = make_pipeline(StandardScaler(), mlp_clf)
y_probas_train_mlp = cross_val_predict(mlp_clf, X_train_in, y_train_in, cv=5,
                                    method="predict_proba")
y_scores_train_mlp = y_probas_train_mlp[:, 1]
print("done!")

"""# RandomForestClassifier

Now, we use a random forest classifier ([RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)) from [Scikit-Learn](https://scikit-learn.org/stable/index.html) [(Pedregosa et al., 2011)](https://www.jmlr.org/papers/volume12/pedregosa11a/pedregosa11a.pdf) to build a classification model.


"""
msg_out = "\nClassification with Random Forest"
print(msg_out,end="...")
from sklearn.ensemble import RandomForestClassifier
forest_clf = RandomForestClassifier(n_estimators=100, max_depth=None,
warm_start = False, random_state=42, oob_score=False,criterion="gini",n_jobs=-1)
from sklearn.model_selection import cross_val_predict
y_probas_train_rf = cross_val_predict(forest_clf, X_train_in, y_train_in, cv=5,
                                    method="predict_proba")
y_scores_train_rf = y_probas_train_rf[:, 1]
print("done!")

"""# SGDClassifier
In this part, we employ the stochastic gradient descent ([SGDClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html)) from [Scikit-Learn](https://scikit-learn.org/stable/index.html) [(Pedregosa et al., 2011)](https://www.jmlr.org/papers/volume12/pedregosa11a/pedregosa11a.pdf) to make a classifier model.
"""
msg_out = "\nClassification with Stochastic Gradient Descent"
print(msg_out,end="...")

from sklearn.linear_model import SGDClassifier
from sklearn.model_selection import cross_val_predict
sgd_clf = SGDClassifier(loss='hinge',penalty="l2",alpha=0.0001,
fit_intercept=True, max_iter=1000,tol=0.001, shuffle=True,verbose=0,
validation_fraction=0.1,random_state=42)
sgd_clf.fit(X_train, y_train_5)
from sklearn.model_selection import cross_val_predict
y_scores_train_sgd = cross_val_predict(sgd_clf, X_train_in, y_train_in,
                            cv = 5, method="decision_function")
print("done!")

"""# ROC Curves and Metrics

In the final part of the code, we generate ROC curves ([Fawcett, 2006](https://doi.org/10.1016/j.patrec.2005.10.010)) and evaluate the predictive performance ([Walsh et al., 2021](https://doi.org/10.1038/s41592-021-01205-4)) of all classifiers.
"""
msg_out = "\nGenerating ROC Curves"
print(msg_out,end="...")
method_list = ["MLPClassifier","RandomForestClassifier","SGDClassifier"]
y_score_train_list = [y_scores_train_mlp,y_scores_train_rf,y_scores_train_sgd]
clf_list = [mlp_clf,forest_clf,sgd_clf]
r_clf = ROC(method_list,X_train_in,y_train_in,y_score_train_list,clf_list,
            X_test_in,y_test_in)
r_clf.performance()
r_clf.curves()
print("done!")